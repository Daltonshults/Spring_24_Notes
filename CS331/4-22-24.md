### Local Search

**Simulated Annealing**: Looks at all of the states that it can get to, chooses one randomly, and if that value increases then it will move to that state. If it doesn't increase then we use a P value to determine if we will go there. According to what the difference is in the value of the objective function. Checks the $\Delta$ between the two points, and if you get to a lesser slope you are closer to a local max/min.
* A random walk takes us in any direction
	* Taking a random walk can end up with you detecting the local max/min
* Example:
	* P = 10
* Uses slope to determine the P value, or the chances of us moving from the current state to the next.

```pseudocode
function simulated_annealing():
	choose n randomly
	if f(n) >= f(s):
		go to n
	else:
		w/ prob p go to n
	otherwise:
		choose another n
```
* Set P to something reasonable:
	* If P is set too low you will never move
	* Setting P too high and you will move around too much
	* This should allow you to get a global max/min
* Probability is set by the $\Delta S_n \thinspace S_{n+1}$ 

Probability:
$$
F(n_1) - f(s)
$$
$$
F(n_2) - F(n_1)
$$

**Beam Search**: In the random restart we are allowed to choose a set of K random states:
* Doing it one after the other
* From K states you can get to all of the states attached to K
* Look at all of the states that you can get to from the set of K and you choose $K\prime$ that has the highest objective function output.
	* Then repeat
* Different from random restart because information is shared

$$
best \thinspace K\prime \thinspace states
$$
**Stochastic Beam Search**: Chooses $K\prime$ random states, starting at K.
* Random: the choice is conditioned on their objection value
	* It rewards having a higher objective function value
* If you have:
	* 7, 3, and 10
	* You would choose 10
	* 10 would be 50% of the total
	* $\frac{10}{(7+3+10)}$
* Doesn't work well in infinite state spaces

### Gradient Descent
* We are looking for the slope of the curve that we are on
* Do not care about the differences in the objective function, but we care about the slope of the curve at the exact point that we are at.
* This is a minimization problem
* Steps:
	* Find the slope of the current state, and go down by calculating the difference between the current state and the slope of the state

$$
x \leftarrow x - \alpha \nabla f(x)
$$
Where:
* $\alpha$ = our learning rate
* f(x) is the derivative of f(x) to find the slope value to determine where we are going to move

Big Concept: The amount you move depends on the slope
* If there is a big slope we are going to take a big step
* If the slope is a small value we are going to take small steps

![[Pasted image 20240422090328.png]]
* $y=x^2$
* $\frac{d}{dx}= 2x$
* If you have a learning rate of 1, and you end up on at (3, 6) you will move to (-3, 6)
* Using 1/2 if you are (3, 6) you would end up at (0, 0)
* Using a small learning rate will cause you to slowly move towards the local minimum, but you will never reach the global minimum
* Works well in infinite state spaces because it allows us to find local minimums 
* Discretizes a infinite space, and with how we define it there is no longer infinite space
* $\alpha$ learning rate determines how many states we allow. By decreasing it we create more states, and increasing it decreases the number of states
* Can work in discrete and continuous environments
* This is how we train Neural Networks
* What happens when we have more that 2-variables or 2-dimensions
* Can quickly do the derivations of them and then it is just algebra to calculate the values
* It is almost impossible to reach a asymptote because of the way we step.
	* We then converge on a number that we want to get close to
* Can do this iteratively, in that you set a point where the $\Delta$ in our values on each iteration is within a specific range, and once you reach the maximum number of iterations without a large enough $\Delta$ change we stop the iterations and return.


### Multi-Agent Environments
* A Multi-Agent Environment:
	* Some of them are competitive

**Competitive Environment:** An environment with multiple agents and their goals conflict with each other.
* Actively working against each other to achieve their goals
* Use adversarial search to look at zero sum games
* Our Environments:
	* Fully observable
	* Deterministic
	* Alternating actions: Turn based environments
	* Discrete states and discrete action sets
	* State space is finite
* Examples: Chess, Go, Nim, football, Battle Ship, and ice hockey
	* A point differential is an example of 0 sum
	* Time limites
	* Can't calculate the optimal decision
* Define a game mathematically:
	* Initial state: the starting configuration of a chess board
		* Know the actions that we can take from this state
	* **player(s)**: the player in the current state that can take a move

S_0 = Initial State \newline
Player(s): The person who can take a move
Actions(s): The actions that a player can take
Results(s, a):State after taking a from s
Terminal Test: fn: game is over
Utility(s, p): In a chess game this is 1 for a win and 0 for a loss
* If the payout is the same every single time it is a zero-sum game

The problem with these state spaces are too large to run a search algorithm on them in a reasonable amount of time
* Tic-tac toe has

Chess: $10^{40}$ states, and searching this number of states makes it hard to find a path through, and in the game of chess, they aren't going to follow the path that you want them to take.

You want to consider if the other player played optimally and prune the search tree of the moves that they will not make.

**Ply**: A single level within a search tree

Mini

#BeamSearch #StochasticBeamSearch #HillClimbing #GradientDescent #LocalSearch 


Homework: The default should be the only one to output to the file. The -A and -B options should print to the console. 
* The file is a required argument
* Use -m for map, and -S for search
