You can say "idk" to one of the problems and still get 20% of the points.
If you guess within 3 points of your score you get extra credit.

### Bayes Networks:
* DAG
* With a dynamically generated probability fields
* Each node contains a probability table 
* Complexity: $O(2^n)$
	* Want to make these systems as simple as possible, meaning we want as few child to parent relationships as possible

$$
G = (V, E)
$$


[[Drawing 2024-05-13 08.39.42.excalidraw]]


Alarm: Burglary and Earthquake

| P(A)  | B   | E   |
| ----- | --- | --- |
| 0.95  | T   | T   |
| 0.99  | T   | F   |
| 0.29  | F   | T   |
| 0.001 | F   | F   |

John Calling:

| P(J) | A   |
| ---- | --- |
| 0.90 | T   |
| 0.05 | F   |
* The probability that John doesn't call can be inferred by the above table
* $P(\neg J)$= 0.1
	* 0.95

Mary Calling

| P(M) | A   |
| ---- | --- |
| 0.70 | T   |
| 0.01 | F   |

Bayes Networks are an encoding of the probability, and represent conditional independence
* The variables become dependent on each other when the alarm is going off
* When there is an Earthquake and the alarm is going off we have a 0.29 probability that it is just an earthquake
* Alarm is conditionally dependent on [[Drawing 2024-05-13 09.00.03.excalidraw]]
	* The node X is conditionally independent of every node in the Bayes Network given X parents, X children, and X's children's parents
	* If we are given the "neighborhood" of X we can say X is independent from the rest of the network
* **Markov Blanket**: This is the parents, children, and children's parents 
	* Easier to compute than the full distribution all the way down
	* Selects pretty much everything that surrounds a specific node
	* Can be relatively simple, and there is a more generalized version
**Something**
$$I(X, Y | E)$$
* Can be computed in linear time
* $X \ \ f \ \ Y \subseteq V - E$ 
* If N is in our evidence, then they are independent of each other, and if N is not in our evidence are dependent on the alarm
* D-separation: By showing that all paths are blocked by one of the cases
	* $N \in E$
	* $N \in E$
	* $N \not\in E$
	* If the path is not blocked they are not de-separated
	* E will be known because we will have data
	* As we get more information nodes can become de-separated

$$
\begin{align*}
 & P(cause | effect) \ \ This \ \ is \ \ more \ \ useful \\
 & P(Effect| Cause) \\
\end{align*}
$$
* We will be building a causal network because we can compute the diagnostics more easily
	* X causes Y and Y causes Z

### **Inference in Bayes Networks:**
* Could be used for classification
	* As the information comes in, the probability that it is or isn't will be adjusted
	* Neural Networks essentially learn a Bayes network, and use that to classify things.
* Types of Inference:
	* Exact Inference
	* Approximate Inference: Direct sampling
		* Markov Chain Monte Carlo Sampling
			* Gibbs Sampling within MCMC
* **Enumeration**
* **Elimination:**
* **Neapolitan**

* Query in a Bayes network:  $P(X | E) = \alpha P(X, E)= \alpha \sum_{y} P(X, E, Y) | P(x_1 ... X_n) = \pi_{i=1}^{n}P(X_i | Parents(x_i))$
	* We are going to sum up the probabilities of all unknown variables to try to find the actual probability
		* We are going to take the product
	* Complexity: $O(n2^n)$
		* Can be improved to $O(2^n)$
	* Satisfiability simplifies to this problem showing that it is NP-Complete

### Monty Hall Problem:

$$
\begin{align*}
& C = \frac{1}{4} \\\\
& D = 1 \\\\
& P(C=1|D=1) = \frac{1}{4}\\\\
& P(C=1) = \frac{1}{4} \\\\
& \frac{1}{4} \times \frac{1}{3} + \frac{3}{4} \times \frac{1}{2} = \frac{11}{24}
\end{align*}
$$
$$
\begin{align*}
& P(C=1 | D=1, H_1=2, H_2=3)\\
& P(H_2) = \frac{1}{4} \times  \frac{1}{2} + \frac{2}{4} \times 2 = \frac{7}{8} \\
\end{align*}
$$

$$
\begin{align*}
& C = \frac{1}{4} \\\\
& D = 1 \\\\
& P(C=1|D=1) = \frac{1}{4}\\\\
& P(C=1) = \frac{1}{4} \\\\
& \frac{1}{4} \times \frac{1}{3} + \frac{3}{4} \times \frac{1}{2} = \frac{11}{24}
\end{align*}
$$
$$
\begin{align*}
& C=D: \frac{1}{4} \\
& C \not=D:  \frac{3}{4} \\
& H_1 \ \ will \ \ have \ \ 2 \ \ doors \ \ \frac{1}{4} \ \ of \ \ the \ \ time\\
& H_1 \ \ will \ \ have \ \ 3 \ \ doors \ \ \frac{3}{4} \ \ of \ \ the \ \ time\\
=& \frac{1}{3} \times \frac{1}{4} + \frac{1}{2} \times \frac{3}{4}\\
=& \frac{11}{24} \\
\end{align*}
$$