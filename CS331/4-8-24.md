Deep blue was based on search, and the Smash Bros model used reinforcement learning, and specifically Q-Learning to achieve it's goals
* This search is what caused Deep Blue to take forever to get its result

### Paper Reading: 
1. First pass: Skim
	1. Read the abstract to understand what the authors were trying to write about
	2. Read the introduction
	3. Skim over the rest of the paper and review the math quickly
	4. Check the visual aids and charts
	5. Read the results more thoroughly
2. Second Pass: Read more carefully
	1. Read through methods, but keep it simple
	2. Take about 7-10 minutes
3. Third Pass: Used to actually digest the paper, and only done if implementation was going to be done. Or the paper is interesting.

* Skips the difficult math
	* Don't need this unless you are talking to the author, or implementing it
* Take an understanding of a the concepts from a high level

### Agents
* You use a agent model to get the base for a learning agent
* Goal based agents: Just try and achieve a goal
* **Problem Solving Agents**: 
	* Have a atomic operations
		* No substates
* An Agent is:
	*  Given a problem, and must return a solution
	* Doesn't know anything about the environment besides the definition of the problem
	* No prior knowledge of the problem
* How to find a solution to the problem from point `A` to point `B`
	* Starts by performing random actions
		* Use to perform uninformed searches:
			* Breadth First Search
			* Depth First Search
		* Informed Searches:
			* Giving an agent more information about the environment
				* This represents prior knowledge
			* More efficient, but difficult to implement because it is hard to "inform" the model
	* Goal:
		* There are a set of states that achieve the goal
			* State `B` solves the goal, but `A` does not
	* Actions:
	* Valid States

Problem: We are trying to get to an airport in Oregon, and it is represented as a cyclic map
* Fully observable
* Action is known- each action has a predetermined outcome of the state
* Solution: Is a fixed sequence of actions
	* Don't have to worry about a specific state or rewards
	* Have a start point and end point, and as long as we execute those actions we don't need to track the state
	* Looking through a sequence of actions is a search
	* Once we have a solution we go through and execute the actions
* Define the problem so that it can be passed to an agent consistently:
	1. **Initial State:** Where we are starting on the map
	2. **Possible Actions:** All the actions that can be taken
		1. Represented as a set of actions that lead from one state to another
	3. **Transition Model**: Defines the actions mathematically
		1. If you take an action from one state to another state you will end up in second state
			1. $RESULT(IN(BEND),\thinspace GO(REDMOND)) \rightarrow REDMOND$
			2. $STATESPACE$ Defines everything we can do in a problem
				1. Goal Test: {B}
				2. Path Cost Function: Sum of the costs of actions
					1. Represented as a directed graph of all the states and the transitions between all the states
					2. ![[Pasted image 20240408091953.png]]
				5. [[Drawing 2024-04-08 09.18.07.excalidraw]]
			3. This represents a abstraction of the problem, and we are expecting the user of the agent to understand how directions work
		2. Example of States of our Vacuum Model
			1. [[VacuumModel.excalidraw]]
	4. **Solutions**: Given a formulated problem find a sequence of actions to achieve a goal, or a path of actions that reach a goal state
		1. **Frontier**: The set of leaf nodes that still need to be explored
			*  Will only search on the frontier
				* Can tell a frontier by keeping an explored set
				* Remove the explored set from the leaf nodes that have already been searched
				* Take a leaf node, decide a frontier, and explore
				* Every search algorithm does a search, but it applies its search differently
			* Searches can be cyclic and redundant to create a cycle
				* want to reduce the chance of redundant or "loopy" paths
					* will expand the search space infinitely or exponentially
			* Each Node: 
				* Parent
				* State
				* Possible Actions
				* Total Cost
					* The total cost is the cost to get to that node from the root
				* Action: That got us from the parent node to the current state
					* Contains all of the information to how we got to this state as well
					* Search algorithms use queue, stack, or priority queue
* Store the nodes that we have already visited into a dictionary/hashmap
				  
				