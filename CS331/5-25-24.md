### Learning
Learning: Improving future performance of some task that is being done
* What we have worked on previously had decisions that we cannot change, and the algorithm decides what behavior is performed.
* Doesn't solve all the problems because the right answer isn't always the same
* **Agents:** learn a mapping of actions to states
	* Learn over a bunch of iterations what action leads to what state
	* Can't see all the results of an action in very large problem states
	* Can learn how to infer certain actions from the percept sequence
	* Learn how the world evolves around us from possible results of an action
	* Learn the value of taking a certain action
		* If an action doesn't always take us to a certain state 
	* Want to learn what the goal states are
		* Learn what a goal state is if it is more complex than just getting from one place to another

### Self-Driving Taxi
* We are a learning self-driving taxi, and we have a passenger who is the instructor
	* Learning why we do things in certain states
		* If it is icy matters when a car is trying to stop
	* Passenger feedback will be considered:
		* If you don't get a tip then you "know" that you did something wrong.
			* You shouldn't slam on the brakes or hit a pedestrian
	* Prior knowledge makes training easier
		* Logic can be used to figure out the right answer to something
		* Can't build a Bayes's Network because it becomes too complex

### Learning:
3 types of learning:
* Unsupervised Learning:
	* Given inputs, but no feedback
	* Clustering is a common version of this
		* Rarely have well defined clusters, and data that doesn't fit into any of the clusters and then the algorithm has a hard time figuring out what to do
* Supervised Learning:
	* Given the input $x$ and the output $f(x) = y$
	* Essentially, this is just a best fit line
	* Can only account for the data that you can see
	* When you have $f(x) = |x|$ and you only had positive values you would predict that the negative values would continue to be linear and be negative, but the graph would be 'V' shaped
* Reinforcement Learning
	* It is kind of between supervised, and unsupervised learning. You give rewards based on actions that lead to different states.
		* Have to look at the data, and make a guess on where it falls, and a reward is given, either negative or positive, based on the distance between the prediction and the actual result
	* Allows you to slowly adjust to what the answer should be based off of the feedback from incorrect guesses
	* Rewards have to be set deliberately: Not too slow, and not too fast.
	* Learn the input/output function based on the rewards that it gets

Semi-Supervised Learning: When you combine supervised and unsupervised learning. You only have a small amount of labels. So you use those to train the model, and then you train in an unsupervised way. 
* You don't have to label everything


### Chess:
* We can use unsupervised learning to have an agent minimize or maximize something
	* Similar to minimax, but minimax doesn't learn.
		* Results in the same exact strategy in the game every single time
* How do we use supervised learning to play chess: You would have to give it the move that it should perform in every state
	* Chess is too large of a game to search with minimax
		* Go is another very large game
	* In chess you can make a move, and get instant feedback
		* Taking a piece or one of our pieces being taken
		* If a sequences outcome is a bad state then the sequence would be scored lower, and we learn that a move is a bad move, or a sequence is a bad sequence
* We can "play" a bunch of games, and see if you win. Over a million games you will eventually learn what is bad. 

What we are going to do:
Assumptions
* look at environments that are fully observable
* we don't know what an action does
* An action does not always lead to the same state

Toy Example: 
Being in a state that is not terminal costs $-0.04$ utility
* $R(s) = -0.04$
* Start at reward of 0
Actions:
* Up
* Down
* Left Right

| \|\| | 1     | 2    | 3   | 4   |
| ---- | ----- | ---- | --- | --- |
| 3    |       |      |     | +1  |
| 2    |       | DEAD |     | -1  |
| 1    | Start |      |     |     |


| \|\| | 1     | 2    | 3   | 4   |
| ---- | ----- | ---- | --- | --- |
| 3    |       |      |     | +1  |
| 2    |       | DEAD |     | -1  |
| 1    | Start |      |     |     |
 Moves: Up, up, right, right, right $\sum R( s) = 0.8$
 * Chose to go up: 80% of the time we will go up 10% of the time we are going left, and 10% we are going right
 * $P(S\prime | s, a)$ Given a state, and an action we are going to end up at a probability
	 * Markov: the history doesn't matter
* Reinforcement learning rewards have to be bounded so you don't have some outlier mess up your utility

Markov Decision Process:
* S: state
* Actions(s) all of the actions from a specific state
* $P(s\prime | s, a)$ an action from one state will lead to a different state
* $R(s)$ Every state has a reward
* $\pi$ : a policy
* $\pi(s)=a$ :  the policy at state $s$ $leads to action $a$
* $\pi *$ : an optimal policy 

How to determine if $\pi *$
* $U(s_1, s_2, s_3) = \sum_{s=1}^{k}R(s)$
* Limit the utility value of far away sequences
* $U(s_1, s_2, s_3) = R(s_0) + \gamma R(s_1) + \gamma^2R(s_2) ... \gamma^k R(s_k)$
	* $0\le\gamma\le1$
	* You can limit what is valued
	* Don't want to look deeper than 5 states into the past

$$
\begin{align*}
	& Bellman \ \ Equation. \\
	U(s) &= R(s) + max( a \in A(s)) \sum_{s\prime} P(s\prime | s, a) U(s\prime)
\end{align*}
$$

* Randomly choose a utility value for each state
	* Still bounding the utility
	* Then update the utility of the current state by running the Bellman Equation 

**Policy Iteration:** Looking at the Bellman Equation with a policy
* We can calculate the estimated maximum expected utility for each action in the policy


How do you evaluate the value of states, and actions!

**Better Way to Do This:**
* The above example is really slow as it explores the same state multiple times and memoization can be used to speed up this process.

Adaptive Dynamic Programming: 

Adaptive Dynamic Programming Agent: ADP Agent
* Uses the bellman equation, and it "plugs in the approximate decision model"
* State and action pair is the input and the utility is the output
* Transition model just keeps a table of probabilities
	* A lot of entries will be 0
	* Will capture the transition model if you have enough simulations
	* Update the probabilities as you get to a state
* ADPA:
	* Returns an action
	* Input a percept: only need the state and reward
	* $\pi$: Policy a representation of the Markov decision process
		* has a model
		* Has different rewards and a $\gamma$ discount value
	* $U$: Table of utilities
		* Will start empty
	* $N_{s, a}$: a table of frequencies of state action pairs
		* Starts everything at 0
	* $S_{s\prime | s, a}$: This is the transition model
	* $s$: state
	* $a$: Action

if $s\prime$ is new:
* Initialize is: 
$$
\begin{align*}
	&U[s\prime] \leftarrow r\prime \\
	& R[s] \leftarrow r\prime \\
	&if\ \ s \ \ is \ \ not\ \ null \\
	&N_{sa} \ \ and \ \ N_{s\prime, \ \ s, \ \ a} \\
	&for \ \ each \ \ t \ \ s.t. \ \ N_{s\prime | s, a}[s\prime, s, a] \\
	&P(t, s, a) \leftarrow N_{s\prime, s, a} \ N_{sa}[S, A] \\  \\
	&U \leftarrow POLICY-EVALUATION(\pi, U, mdp) \\
	&\quad \quad if \ \ s\prime \ \ is \ \ terminal: \ \ s, a, \leftarrow null \ \ \\
	&\quad \quad else \ \ s, a \leftarrow s\prime, \pi(s) \\ 
	&\quad \quad return \ \ a
\end{align*}
$$

We should be able to know constraints up front that will allow us to estimate the utilities faster
* If we know our estimate is off we should be able to adjust it with our knowledge

Update with knowledge:
$$
\begin{align*}
U^\pi(S) \leftarrow U^\pi(s) + \alpha(R(s) + \gamma U^\pi(s) - U^\pi(s))
\\
\end{align*} \\ 
$$

TDAgent:
$$
\begin{align*}
&TDAgent: \ \ returns \ \ a \ \ input: s\prime, r\prime \\ 
&vars: \pi, U, N_s, S, a, r \\
&if \ \ s\prime \ \ is \ \ new : U[s] \leftarrow r\prime \\
&if \ \ s \ \ is \ \ not \ \ null: \\
&\quad increment \ \ N_s[s] \ \ \\
&\quad  U[s] \leftarrow U[s] + \alpha(N_s[s])(r+\gamma U[s\prime]-U[s])\\
&\quad if \ \ s\prime \ \ is \ \ terminal: s, a, r \leftarrow null \\
&\quad else: \ \ s, a, r \leftarrow \prime, \pi(r), r\prime \\
\end{align*}
$$
TD: Is an approximation of ADP
* Both are passive reinforcement learning
* Learn the utility of a given policy, and don't alter the policy that we are looking at

Wednesday: We will look at the active reinforcement learning algorithms
* These have to learn the utility and transition models