### Current Events:
* AMAF and RAVE are biased because if you make a good move in a previous turn it 
* Monte Carlo Tree Search: The benefit of this is that we have already pre-computed a ton of weights, but they are not optimal. The parallelism allows us to build the policy quickly, and the policy can be used to weight the value of certain actions that place us in a specific state.
	* Policy allows for pruning
	* Stochastic: It allows us to make sure that our policy isn't biasing our decision process 
* Randomness helps this search a lot, being able to make a random choice and evaluating it is important for creating policies. This allows us to make better random choices further down the decision tree

### Logic:
* Sound:
* Complete:
* Entailment, and Inference should be the same
	* If we can show that something is entailed we can infer it

### Propositional Logic
* **Propositional Logic:** Contains sentences 
	* Sentence: Can one of the two different types of sentences
		* Atomic Sentence: A single literal, or a single symbol, and are used to construct complex sentences.
		* Complex Sentence: $\neg \thinspace \alpha$
			* Example Complex Sentences:
				* $\neg \thinspace \alpha$ : negation
				* $\alpha \land \beta$: Conjunction
				* $\alpha \lor \beta$: Disjunction
				* $\alpha \Rightarrow \beta$: Implication
				* $\alpha \Leftrightarrow \beta$: Bi-conditional
			* PEMDAS: $\neg, \lor, \land, \Rightarrow, \Leftarrow \Rightarrow$
		* Symbols: $P|Q|R$
	* Example:
		* Every model must have a truth symbol for each model
		* T,       T,       F
		* $\neg A \land B \Rightarrow C$ 
			* $\neg A \space if \space A$  is true
Implication Truth Table: Either A has to be false, or B has to be true

$A \Rightarrow B$

|     | B   |     |
| --- | --- | --- |
| A   | T   | F   |
|     | T   | T   |
$A \Leftrightarrow B$

| A/B   | **T** | **F** |
| ----- | ----- | ----- |
| **T** | T     | F     |
| **F** | F     | T     |

Theorem proving is more straight forward that model checking.
* doesn't require us to enumerate all of the different models
* If you have a ton of symbols it could make this work in an inefficient way
* Craft a proof with a simple model, you are given all the logical sentences in the knowledge base
* Commutativity:
	* $A \land B \equiv B \land A$
	* $A \lor B \equiv B \lor A$
* Exclusivity:
	* $(A \land B) \land C \equiv A \land (B \land C)$
	* $(A \lor B) \lor C \equiv A \lor (B \lor C)$
* Distributivity:
	* $A \land (B \lor C) \equiv (A \land B) \lor (A \land C)$
	* $A \lor (B \land C) \equiv (A \land B) \land (A \lor C)$
* Double Negation
	* $\neg (\neg A) \equiv A$
* $A \Rightarrow \equiv \neg B \Rightarrow A$
* $A \Rightarrow B \equiv \neg B \Rightarrow \neg A$
* $A \leftrightarrow B \equiv (B \Rightarrow A)$
* DeMorgans Law
	* $\neg (A \land B) \equiv \neg A \lor \neg B$
	* $\neg(A \lor B) \equiv \neg A \land \neg B$

Valid: $\alpha$ is valid iff
* $\forall$ means $\alpha$ is true
	* $A \lor \neg A$

Deduction Theorem:
* For any 2 set $\alpha, \beta$
* $\alpha \models \beta \space iff \space \Rightarrow \beta \space is \space valid$
* Satisfiability: The logical condition can be met
	* $\alpha \exists\ \ m\ \ s.t.\ \ \alpha \space is \space true \space in \space m$
	* $\alpha \models \beta$  iff
*  Entailment: $\alpha \models \beta \   \ iff \  \ \neg Satisfiable(\alpha \land \neg\beta)$

Proof by contradiction: Pick an example assume that it is true, and find that there is no way that is not possible within the world

Modus-Ponens: SOUND
* $\frac{A \Rightarrow B, A}{B}$
* If A implies B and A is Alpha we can imply B

Knowledge Base:
* Wumpus_Alive
* Wumpus$_{1, 3}$
* Agent$_{1, 2}$ 
* AgentDir$_{up}$
Sentence: Wumpus alive and Wumpus ahead
* $\alpha \equiv$ We could have a policy that if the Wumpus is alive, and ahead of us
	* $\frac{\alpha \Rightarrow shoot, \alpha}{\beta}$

	
Elimination: SOUND if we apply them we don't lose validity in our proof
* $\frac{A \land B}{B}$

A knowledge base cannot have two rules that contradict each other, and the knowledge base does not change.
* Cannot say "Patrick loves horses," and then add in "Patrick hates horses"


InitialState: $KB$
Actions: Applying an sentence to a sentence in our $KB$
Result: New alpha added to  and sentence $\exists$ $KB$
Goal: State containing $\alpha \land \neg \beta$
* you are aiming for the implication of the above sentence
Complete Problems: Must be able to be completed if the rules allow you to find the negative implication of $\beta$

Resolution: Completes the theorem prover, and is simple in practice, but becomes complicated easily

$A \neg A$
$A \epsilon \alpha$
$\neg A \epsilon \beta$

$A \land B \land C$
$\neg A$
========
$B \lor C$

Resolution: Taking two clauses that are the same, and making them a single clause
$A \lor B$
$\neg A \lor B$
======
$B \lor B$ -> Using factoring
$B$
Can use the above sentences to create a new clause or sentence

Unit Resolution: For some string of literals, and some complimentary literal m we can infer; 
$\frac{\lambda}{v ...}$

Full Resolution: For two sentences, we can remove complimentary literals, and then or the rest of the values together.
* Because we have a complimentary literal, we can remove them because they do not effect the result, and the resolvent must be true.

Proving Entailment:
$$A \Rightarrow \neg B \models \neg ( B \land A)$$
$$\neg A \lor \neg B \models \neg(B \land A) $$
$$\neg(A \land B) \models \neg(B \land A)$$
$$\neg(B\land A) \models \neg(B \land A)$$

