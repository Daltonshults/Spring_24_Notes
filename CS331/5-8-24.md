* If you have n variables you need $2^n$ size table
* Independence makes our tables smaller, and allow us to scale


$$
P(A, \ \ B, \ \ C)
$$
$$P(a_i, \ \ b_k, \ \ c_i)$$
$$P(\neg a_i, \ \ b_k,\ \ c_i)$$
**Conditioning**: we can sum the probabilities where the thing is true, and we can try to figure out what the value of the variables are to reduce the number of rows that we need to calculate

If $a$ and $b$ are not independent, then we cannot just remove P(b) from the formula:
$$\frac{P(a \land b)}{P(b)} $$

$$P(a|b) = \alpha P(a, b)$$
b- can represent more than a single probability, but instead could be an entire clause made up of different propositions.
* If it does change the same over time we don't want to normalize, but if it does not you can normalize it.
* We want to make things infer from what we know, and this will look like logic

**Evidence Variables:** 

$$Let \ \ E \ \ be \ \ a \ \ set \ \ of \ \ propositions$$
$$Let \ \ y \ \ be \ \ the \ \ remaining \ \ unobserved \ \ variables$$
What we want to find:
$$P(x|e)=\ \ \alpha P(X|e)\ \ = \alpha \sum_{y\in Y}P(X,\ \ e,\ \ y)$$
Ruling out the rows that $e$ is false, and all the rules that don't agree with how $y$ is set
* Sum out all the probabilities of $X$ for all the rows the $e$ is true and $y$ is true
* Going through all the possible unobserved worlds

$$\alpha \sum_{y\in Y}P(X, e, y)$$
* Has the time complexity of $2^n$
* Impossible to store the whole distribution
	* If we know that two variables are independent we can just store their probabilities independently

$$X\ \ and\ \ Y \ \ are \ \ independent \ \ iff$$
$$P(x|y)=P(x)$$
and 
$$P(y|x)=P(y)$$
equals: product rule
$$P(x, y) = P(x)P(y)$$
* This allows us to store two rows for the leaving us with a complexity of $2n$
	* Can be extended to large systems, and this allows us to do it at scale

$$P(A, \ \ B)=P(B|A)P(B)$$
$$P(A,\ \ B) = P(A|B)P(A)$$
Can rewrite as: **Bayes' Rule**
$$P(B|A)P(A)=\frac{P(A|B)P(B)}{P(A)}$$

Generalized Bayes' Rule: We added the "evidence"
$$P(Y|X, e)= \frac{P(X|Y, e)P(Y, e)}{P(X|e)}$$
Example: Probability of having a cold given a cough
$$P(A)=0.02 \ \ Cough$$
$$P(B)=0.1 \ \ cold$$
$$P(A|B)=0.7 \ \ Cough \ \ and \ \ cold$$
$$= \frac{(0.7)(0.1)}{(0.05)}$$
$$=0.35$$
As they get evidence they can increase, or decrease the probability of a "cold" (or whatever it is the probability for)

**Diagnostic Direction**: This can be written as 
$$P(Cause \ \ | \ \ Effect) = \frac{P(effect|cause)P(cause)}{P(effect)}$$
Diagnosing a cause given an effect
Causal Direction : $$P(effect|cause)$$
* Based on evidence, you can tell if someone is more likely to have a cold if they have a cough
* Based on statistics they know the causal, they know that a cough could cause a cold, cancer, flu, etc.
* Rely on evidence

**Diagnostic Direction**: $$P(Cause|Effect)$$

What happens when we have a lot of evidence variables?
* Cough
* Shortness of breath
* headache
Look at the symptoms independent from the cause
* Real life has variables that are all independent of each other when conditioned on the cause, you can rewrite Bayes' rule as

$$P(Covid|Cough\land Headache) = \alpha P(cough|Covid) P(Headache|Covide) P(Covid)$$
$$\alpha P(A_1|B)P(A_2|B)P(B)$$
* $\alpha$ is normalizing the probabilities for us

$$Condition|Independence$$
$$X, Y | Z$$
$$P(X, Y | Z) = P(X|Z)P(Y|Z)P(Z)$$
Or
$$P(X|Y, z) = P(X|Z)\ \ and \ \ P(Y|X, Z)= P(Y|Z)$$
* What this means, is if we know the value of Z we don't need it to get the value of x, or y

$$P(X, Y, Z) \ \ 2^n$$
$$P(X|Z)P(Y|Z)P(Z) \ \ 2n$$
* When we have 10 variables the top value would be 1024, and the bottom would be 20

Naive Bayes':
* One Cause, and a bunch of effects
* When all of the effects are independent of the cause
$$P(Cause|E_1 ... E_k) = P(cause) \pi_{i=1}^{k} P(E_i |Cause)$$
* They are all conditionally independent from each other when they 
* Bayes works as a representation of the full tables, and they keep track of the independent conditional statement

Bayes Network: A directed acyclic graph
* Acyclic graph is a tree
* DAG: Directed acyclic graph
* Node: represents a random variable in our system
* Edge: Connects two random variables, but it implies that (x, y) that x is a parent of y. 
	* y is dependent on x (if you have a cough you could have covid)
	* x causes y (covid causes a cough)
* Note: $X_i$ has a conditional product distribution given its parents
* $$P(X_i|Parents(X_i))$$
Truth Table

|       | Covid |     |     |
| ----- | ----- | --- | --- |
| Cough | X     | T   | F   |
|       | T     | 0.7 | 0.2 |
|       | F     | 0.3 | 0.8 |
If we know it is boolean we only need to track

|       | Covid |     |     |
| ----- | ----- | --- | --- |
|       | T     | F   |     |
| Cough | 0.7   | 0.2 |     |
|       |       |     |     |
* You can do this because $$1-p(a)=p(\neg a)$$