### Agent
* Perceive the environment through sensors
* Actuator: The way an agent is able to perform an 
* Environment: The domain in which the agent is acting
* If we are laying chess what the agent perceives:
	* Pawn to E3
		* A single action
	* Pawn to E3
* Percept Sequence
* Policy: What is developed to guide the agent's actions 
	* Can be simple or complex
	* Defined relative to the percept sequence
* Agent Program: The actual implementation of a policy
	* Can create practical differences 

**Percept Sequence:** Everything the agent has seen within the environment

Example of Roomba Policy: If the square I am in clean
* If the square is clean move right or left

Rationality is relative to how we define it, and how the policy is executed can determine if it is rational

**Performance Metric**: Rationality is relative to the performance metric, or how we measure how effective the agent is acting within the environment
* Don't care how well the Roomba thinks it is clean, it only matters if it is actually clean
* Performance metrics should monitor the environment, and not how the agent thinks it is doing
* Depends on the **prior knowledge** of the environment that we give to the agent
	* Example: Prior games of chess that they have played
		* How dirty the environment is for a Roomba
* The **actions** that an agent can take
* Percept sequence

Rational Agent: For every percept sequence they maximize the actions they take given the all the current information the model knows
* It always tries to take the best action based on what it knows

**Roomba Rational**: 
* What is dirty and clean
* How long are the squares going to remain dirty

**How to deal with time**:
* Discrete Time: Saying each action is going to take a single time step
* Continuous Time: Each action takes a certain amount of time

**Agent Learning**:
1. Prior knowledge
2. Computation of Actions: The agent computes a probabilistic function to decide how it is going to work. Doesn't change how the probabilities work
3. Reinforcement Learning: The agent can learn from it's own experience. The agent did X and it caused Y. Measuring the success of Y will give us information on the benefits of X.
	* Can change on how it acts depending on the percept sequence
	* Changes the probabilities based on weighting the decision trees based on the results of previous actions
	* Takes into account what the environment is actually like compared to it is perceived
	* Autonomous agent learns from its environment

#### How to Frame a Problem?

* **Task Environment** Consists of:
	* **P**erformance Metric: Measuring how well the agents actions
	* **E**nvironment: What do we have to interact with or account for
	* **A**ctuators: The ways an agent can take action
	* **S**ensors: How the agent perceives the environment

Self Driving Car  **PEAS** Examples:
* **Performance**
	* Primary Performance Metric: Did you get from point **A** to point **B**
	* Minimize collisions 
	* Minimize the amount of time spent driving
	* Minimize the energy consumption
* **Environment**
	* Roads
	* Cars
	* Pedestrians
	* Wildlife
	* Weather
	* Signs
* **Actuator**:
	* Steer
	* Gas Pedal
	* Break
	* Turn signals
* **Sensors**:
	* Cameras
	* Radar
	* Sonar
	* GPS

#Environment 


**Observability**:
* Fully Observable: You can detect and sense every aspect of the environment
	* Have a high degree of control over the environment
* Partially Observable
	* Most common
	* Can't see everything in the environment
		* A self driving car can only see what is not being blocked by other objects within the environment
* **Deterministic Environment**: $F(S_i, A) \rightarrow S_{i+e}$
* **Stochastic Environment**: $F(...) \not\to S_{i+z}$
* **Episode**: One state, one action
* **Sequential**: Every action you take affects all of the states ahead of you. 
	* Butterfly affect
	* A single move in chess changes how the rest of the game is going to be played
	* Most agents are sequential
* **Static Environment**: While we are deciding the action we are going to take the environment does not change.
	* Chess
	* Turn based games
* **Dynamic Environment**: While decisions are being made the environment can change
	* Self-driving cars: The environment can constantly be changing while decisions are being made
* **Discrete Datatypes:** A limited number of datatypes that can be in your environment 
	* Often people will try to create discrete data because the computational expenses are lower
* **Continuous Datatypes**: An environment that can have an unlimited number of datatypes
* **Single Agent**: When a single agent is taking the actions in the environment
* **Multiple Agents**: When you have multiple agents acting within the same environment
	* A person playing against an AI bot is an agent

##### Policy:
* **Simple Agent**: table lookups
	* Given a percept sequence perform a specific action
	* Creates a massive table
* **Simple Reflex**: It sees something and it uses a bunch of if/else blocks
	* The environment changes the agent takes an action
	* No learning involved
* **Model-Based Reflex Agent**: Models the state of the environment, and keeps track of the percept sequence.
	* Tracks: Environment, and percept sequence
		* As the percept sequence comes in the state is updated
	* Uses the model of the state, and the percept sequence to make decisions
* **Goal-Based Agent:** Given a goal takes actions towards the goal using the performance metric to measure how well the goal was achieved
	* Can use an entire or limited percept sequence
	* Can reason about what the future states will be
		* If action X is taken what are the Y states that could come back to me, and of the Y states how will these affect future outcomes
	* Use a breadth/depth first search to find this information
		* Searches the possible pool of states, and the states are connected to an action, and a path to completing the goal is made through a sequence of actions
* **Utility-based Agent**: Search for the path with the highest chance of achieving the goal
	* **Utility** Can measure the chances of achieving a goal given a set of actions that can be taken. Chooses the action with the highest chance of achieving the goal.
	* Measures the expected utility because you have to know the actions of all other agents within the environment to know the actual utility.
	* Can't do this for the entire game of chess at the start, but can approximate it using probabilities
* None of the above learns how to play the game, but it chooses the best path to achieve their goal
* **Learning Agents**: Use a critic to learn how to achieve their goals
	* Critic: An opening to the performance metric
		* Knows how well an action will work
		* Gives feedback immediately
			* Says that is a good move or a bad move, or a score between 0-1
			* Feedback is then used to adjust the weighting of the utility
	* Similar  to utility based agents, but get feedback and update the probabilities
		* The update can or will change the expected utilities of actions
	* If you have it do enough of these tasks it will learn what the right things to do are, and will develop a policy
	* Simulation can help develop policies for learning agents in environments that the consequences are too high
		* A self-driving care hitting someone

Rational Agents: Care about the performance metrics
#PEAS #PerformanceMetric #Environment #Agents #Datatypes #Policy #RationalAgent #LearningAgent #UtilityBasedAgent #GoalBasedAgent #SimpleReflexAgent #ModelBasedAgent #SimpleAgent