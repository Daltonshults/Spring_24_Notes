### Reinforcement Learning
* Using the grid world from last class
* Environment is fully observable but we don't know what an action does
	* Action a does not always lead from $s\rightarrow s\prime$
 
3 Different kind of agents:
* Utility Agent: learns the utility of a state
* Q-Learning Agent: Learns an action utility function
	* Learns what utility a specific action results in
* Reflex Agent: Given a state take an action

### Utility Based Agent:
* Needs to understand its environment to know all of the states it can go to
	* Q-learning has an advantage because it is only tracking actions and not the number of states
	* Q-Learning doesn't use look-a-head
* **Two types of reinforcement learning**:
	* **Passive**: Takes a fixed policy and learns the utility of states (only utility)
		* Can do Q-learning in this case you are learning the utility of actions
			* Not deciding what action to take
		* Similar to our policy iteration
		* Have to figure out what the transition model is
			* Can only approximate this
	* **Active**: Learn the utility and states, and learns what actions to take in specific states
		* Changes the policy
		* Don't know what each action does, and does not know what the results of each action are going to be.
		* Exploitation: Taking actions that the model knows what it will do, and knows what the expected reward will do.
		* Explore: You have to explore otherwise the model will just repeat the first action that works
		* Doesn't know what the transition model or reward function

Execute the policy that we are given and it will give us a reward state
* Gives us a set of states, actions, and rewards 
	* Each action will be learned on how it works in
	* Will slowly converge to the right answer as you do more trials
* Goal: Learn the utility
$$U^\pi(s) \ \ \forall s \in S$$

U: utility
$\pi$: Policy
s: single state
S: all the states

$$
\begin{align*}
	U^\pi&= E[\ \ \sum_{t_0}^{\infty}\gamma^t R(s_t)\ \ ]
\end{align*}
$$ 
E: Expected
R: Reward function
$s_t$: represents a state with some probability
Utility: is the sum of the expected reward

We are going to run a bunch of trials, and we get a bunch of sequences of pairs, and we take each of them and average the rewards to get the policy
* Essentially supervised learning because we aren't guessing anything. We aren't learning the transition model. We have a simulation and the expected utility for the state is the average of the utility of each simulation.
	* State is the input
	* Utility is the output
* Missing one key piece: The states in each trial, and the utility for each state is not independent of each other.
	* Direct utility association is bad because it will treat them as if they are independent.