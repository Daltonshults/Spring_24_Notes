### Rate Limiting
* Why?
	* The server is going to do less work if you implement rate limiting
	* They don't want you to steal their data
* HOW?!?!?!
	* Track the people, or computers over time. With the number of uses of the website over time.
	* Identify users by username (if logged in)
	* OR by IP address
	* Want to measure this by timeframe:
		* We are only going to allow so many requests in a set amount of time
			* Normally you allow a burst of requests because this is more similar to how the web is used than having an evenly distributed time per request
				* You can do 10 requests every 10 seconds, but no more than 10
### The Bucket Algorithm
* Bucket Algorithm
	* You get a bucket of tokens
	* each request gives us a token
	* Tokens are replenished at a fixed rate
	* The bucket has a max capacity

Example:
* Start off with 10, and each request you make takes a single token out of the bucket
* Only replenish buckets when a user makes a request
	* When a request is made you check when their last request was made, and use that value to generate the tokens to replenish the bucket

Math:
* Given:
	* Capacity
	* Refresh Rate: requests per second `requests/second`
	* Initial Value of the bucket
* When we get a request:
	* Want to know how many more tokens we should add to the bucket
	* How many tokens should we add to the bucket 
	* Get elapsed type: `cur_req_time - prev_req_time`


```
elapsed = cur_req_time - prev_req_time

tokens += elapsed * refresh_rate

if (tokens > max_tokens) tokens = max_tokens

if (tokens > 0) {
	handle_request_normally()
	tokens --
} else {
	respond with 429
}
```
* Will end up with fractional tokens
* The math works better for milliseconds because the functions are built to use milliseconds

How to track previous request time

```
user_ip = req.ip

const ip_to_user = {}

prev_req_time = ip_to_user[user_op]

elapsed = cur_req_time - prev_req_time

tokens += elapsed * refresh_rate

if (tokens > max_tokens) tokens = max_tokens

if (tokens > 0) {
	handle_request_normally()
	tokens --
} else {
	respond with 429
}
```

Using token data:
```js
const ip_to_user = {}

user_ip = req.ip

prev_req_time = ip_to_user[user_op]
user_bucket_data = ip_to_user[user_ip]
if (!user_bucket_data) {
	ip_to_user[user_ip] = {
		"tokens": "initial token value"
		"last": Date.Now() // MS
	}
}
elapsed = cur_req_time - prev_req_time

user_bucket_data.tokens += elapsed * refresh_rate
user_bucket_data.last = cur_req_time

if (tokens > max_tokens) tokens = max_tokens

if (tokens > 0) {
	handle_request_normally()
	tokens --
} else {
	respond with 429
}
```


As a function
```js
function rateLimit(req, res, next) {
	const ip_to_user = {}
	
	user_ip = req.ip
	
	prev_req_time = ip_to_user[user_op]
	user_bucket_data = ip_to_user[user_ip]
	if (!user_bucket_data) {
		ip_to_user[user_ip] = {
			"tokens": "initial token value"
			"last": Date.Now() // MS
		}
	}
	elapsed = cur_req_time - prev_req_time
	
	user_bucket_data.tokens += elapsed * refresh_rate
	user_bucket_data.last = cur_req_time
	
	if (tokens > max_tokens) tokens = max_tokens
	
	if (tokens > 0) {
		handle_request_normally()
		tokens --
	} else {
		respond with 429
	}
}

// can be called by
app.get(rateLimit (req, res, next) => {
});

// or
app.use(rateLimit);
// Makes this universal and applies it to every view
```
